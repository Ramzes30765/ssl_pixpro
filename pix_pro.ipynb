{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kitt/ssl_pixpro/.ssl/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.attention import flex_attention\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "import timm\n",
    "from clearml import Task, Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from omegaconf import OmegaConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_task(cfg_path):\n",
    "    cfg = OmegaConf.load('configs/config.yaml')\n",
    "\n",
    "    train_task = Task.init(task_name='123')\n",
    "    train_task.connect_configuration(cfg, name=\"config\")\n",
    "\n",
    "    train_task.add_parameter(\"train.epoch\", cfg.train.epoch)\n",
    "    train_task.add_parameter(\"train.learning_rate\", cfg.train.learning_rate)\n",
    "    print(f\"EPOCH = {cfg.train.epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = Task.init(task_name='123')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = OmegaConf.load('configs/config.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dict(cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pipeline': {'pipe_name': 'SSL pipeline', 'pipe_proj_name': 'PixPro'},\n",
       " 'task': {'proj_name': 'PixPro', 'task_name': 'ResNet'},\n",
       " 'model': {'backbone': 'resnet18', 'pretrained': False, 'projector_blocks': 1, 'predictor_blocks': 1, 'reduction': 4},\n",
       " 'data': {'img_size': 640, 'dataset_name': 'ssl_turbine_dataset', 'train_folder': 'turbine_train', 'val_folder': 'turbine_val', 'batchsize': 32, 'numworkers': 16},\n",
       " 'train': {'epoch': 1, 'lr_start': 0.001, 'lr_end': 1e-05, 'devices': 'auto', 'accelerator': 'auto', 'val_step': 10, 'log_step': 5},\n",
       " 'val': {'eps': 0.5, 'min_samples': 5, 'sample_fraction': 1.0}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pipeline': {'pipe_name': 'SSL pipeline', 'pipe_proj_name': 'PixPro'},\n",
       " 'task': {'proj_name': 'PixPro', 'task_name': 'ResNet'},\n",
       " 'model': {'backbone': 'resnet18', 'pretrained': False, 'projector_blocks': 1, 'predictor_blocks': 1, 'reduction': 4},\n",
       " 'data': {'img_size': 640, 'dataset_name': 'ssl_turbine_dataset', 'train_folder': 'turbine_train', 'val_folder': 'turbine_val', 'batchsize': 32, 'numworkers': 16},\n",
       " 'train': {'epoch': 1, 'lr_start': 0.001, 'lr_end': 1e-05, 'devices': 'auto', 'accelerator': 'auto', 'val_step': 10, 'log_step': 5},\n",
       " 'val': {'eps': 0.5, 'min_samples': 5, 'sample_fraction': 1.0}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task.connect_configuration(dict(cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cfg = task.get_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "new_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timm.list_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelPropagationModule(nn.Module):\n",
    "    def __init__(self, in_channels, reduction=4):\n",
    "\n",
    "        super(PixelPropagationModule, self).__init__()\n",
    "        self.inter_channels = in_channels // reduction\n",
    "        self.query_conv = nn.Conv2d(in_channels, self.inter_channels, kernel_size=1)\n",
    "        self.key_conv   = nn.Conv2d(in_channels, self.inter_channels, kernel_size=1)\n",
    "        self.value_conv = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.size()\n",
    "        proj_query = self.query_conv(x).view(B, self.inter_channels, -1).permute(0, 2, 1)  # [B, H*W, C]\n",
    "        proj_key   = self.key_conv(x).view(B, self.inter_channels, -1) # [B, C, H*W]\n",
    "        score = torch.bmm(proj_query, proj_key) # [B, H*W, H*W]\n",
    "        attention = F.softmax(score, dim=-1) # [B, H*W, H*W]\n",
    "        proj_value = self.value_conv(x).view(B, C, -1) # [B, C, H*W]\n",
    "        out = torch.bmm(proj_value, attention.permute(0, 2, 1)) # transpose attention - [B, C, H*W]\n",
    "        out = out.view(B, C, H, W)\n",
    "        out = self.gamma * out + x\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlexAttentionPPM(nn.Module):\n",
    "    def __init__(self, in_channels, dropout_p=0.0, is_causal=False):\n",
    "\n",
    "        super(FlexAttentionPPM, self).__init__()\n",
    "        # Проекция для формирования Q, K, V в один шаг\n",
    "        self.qkv_proj = nn.Conv2d(in_channels, in_channels * 3, kernel_size=1)\n",
    "        self.dropout_p = dropout_p\n",
    "        self.is_causal = is_causal\n",
    "        # Обучаемый коэффициент для остаточного соединения\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        B, C, H, W = x.size()\n",
    "        qkv = self.qkv_proj(x)\n",
    "        q, k, v = torch.chunk(qkv, chunks=3, dim=1)  # каждый [B, C, H, W]\n",
    "        \n",
    "        q = q.view(B, C, -1).permute(0, 2, 1)  # [B, H*W, C]\n",
    "        k = k.view(B, C, -1).permute(0, 2, 1)  # [B, H*W, C]\n",
    "        v = v.view(B, C, -1).permute(0, 2, 1)  # [B, H*W, C]\n",
    "        \n",
    "        attn_out = flex_attention(query=q, key=k, value=v, \n",
    "                                  attn_mask=None,\n",
    "                                  dropout_p=self.dropout_p,\n",
    "                                  is_causal=self.is_causal)\n",
    "\n",
    "        attn_out = attn_out.permute(0, 2, 1).view(B, C, H, W)\n",
    "        out = self.gamma * attn_out + x\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = timm.create_model('resnet18', pretrained=False, features_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixPro(nn.Module):\n",
    "    def __init__(self,\n",
    "                 backbone_name,\n",
    "                 pretrained=False\n",
    "                 ):\n",
    "\n",
    "        super(PixPro, self).__init__()\n",
    "        self.backbone = timm.create_model(backbone_name, pretrained=pretrained, features_only=True)\n",
    "        self.in_features = self.backbone(torch.randn(1, 3, 1024, 768))[-1].shape[1]\n",
    "        self.proj_dim = self.in_features * 4\n",
    "        self.hidden_dim = self.in_features // 4\n",
    "        \n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Conv2d(self.in_features, self.proj_dim, kernel_size=1),\n",
    "            nn.BatchNorm2d(self.proj_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(self.proj_dim, self.proj_dim, kernel_size=1)\n",
    "        )\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Conv2d(self.proj_dim, self.hidden_dim, kernel_size=1),\n",
    "            nn.BatchNorm2d(self.hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(self.hidden_dim, self.proj_dim, kernel_size=1)\n",
    "        )\n",
    "        self.pixel_propagation = PixelPropagationModule(self.proj_dim, reduction=4)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "\n",
    "        f1 = self.backbone(x1)[-1]  # [B, in_features, H, W]\n",
    "        f2 = self.backbone(x2)[-1]\n",
    "        \n",
    "        z1 = self.projector(f1)     # [B, proj_dim, H, W]\n",
    "        z2 = self.projector(f2)\n",
    "        \n",
    "        p1 = self.predictor(z1)     # Предсказания (ветвь, по которой обновляются веса)\n",
    "        p2 = self.predictor(z2)\n",
    "        \n",
    "        y1 = self.pixel_propagation(z1)  # Целевые представления (для target)\n",
    "        y2 = self.pixel_propagation(z2)\n",
    "        \n",
    "        return p1, p2, y1, y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pixpro_loss(p1, p2, y1, y2):\n",
    "    # Flatten по пространственным измерениям: [B, proj_dim, H, W] -> [B, proj_dim, H*W]\n",
    "    p1_flat = p1.flatten(2)\n",
    "    p2_flat = p2.flatten(2)\n",
    "    y1_flat = y1.flatten(2)\n",
    "    y2_flat = y2.flatten(2)\n",
    "    # Вычисляем негативное косинусное сходство\n",
    "    loss1 = -F.cosine_similarity(p1_flat, y2_flat.detach(), dim=1).mean()\n",
    "    loss2 = -F.cosine_similarity(p2_flat, y1_flat.detach(), dim=1).mean()\n",
    "    return 0.5 * (loss1 + loss2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_augmentations(image_tensor):\n",
    "\n",
    "    pil_img = transforms.ToPILImage()(image_tensor)\n",
    "    \n",
    "    augmentation = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224, scale=(0.2, 1.0), ratio=(0.75, 1.33)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomApply([\n",
    "            transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1)\n",
    "        ], p=0.8),\n",
    "        transforms.RandomGrayscale(p=0.2),\n",
    "        transforms.GaussianBlur(kernel_size=23, sigma=(0.1, 2.0)),\n",
    "        # (Опционально) Solarize: можно раскомментировать, если эксперименты показывают пользу\n",
    "        # transforms.RandomSolarize(threshold=128, p=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        # Нормализация, если используется предобученный backbone с ImageNet нормализацией\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    return augmentation(pil_img)\n",
    "\n",
    "def batch_augmentations(batch_images):\n",
    "    \"\"\"\n",
    "    Принимает батч изображений в виде тензора [B, C, H, W] и возвращает батч аугментированных изображений.\n",
    "    \"\"\"\n",
    "    augmented_images = []\n",
    "    for img in batch_images:\n",
    "        augmented_images.append(advanced_augmentations(img))\n",
    "    return torch.stack(augmented_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pixpro(model, dataloader, optimizer, device, epoch, augment_fn=batch_augmentations):\n",
    "    \"\"\"\n",
    "    Обучает модель PixPro на одной эпохе.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    task = Task.current_task()\n",
    "    for batch_idx, (images, _) in enumerate(dataloader):\n",
    "        x1 = augment_fn(images.clone())\n",
    "        x2 = augment_fn(images.clone())\n",
    "        x1, x2 = x1.to(device), x2.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        p1, p2, y1, y2 = model(x1, x2)\n",
    "        loss = pixpro_loss(p1, p2, y1, y2)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"Epoch {epoch} Batch {batch_idx}/{len(dataloader)} Loss: {loss.item():.4f}\")\n",
    "            task.get_logger().report_scalar(\"Training\", \"Loss\", iteration=epoch * len(dataloader) + batch_idx, value=loss.item())\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"=== Epoch {epoch} Average Loss: {avg_loss:.4f} ===\")\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dense_features_global(model, dataloader, device, sample_fraction=1.0):\n",
    "    \"\"\"\n",
    "    Извлекает dense признаки для всего датасета: из последней карты признаков, преобразованной в форму [B*H*W, C].\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_features = []\n",
    "    with torch.no_grad():\n",
    "        for images, _ in dataloader:\n",
    "            images = images.to(device)\n",
    "            feat_maps = model.backbone(images)[-1]  # [B, C, H, W]\n",
    "            feat_maps = F.normalize(feat_maps, p=2, dim=1)\n",
    "            B, C, H, W = feat_maps.shape\n",
    "            features = feat_maps.view(B, C, -1).permute(0, 2, 1).contiguous().view(-1, C)\n",
    "            if sample_fraction < 1.0:\n",
    "                num_samples = int(features.size(0) * sample_fraction)\n",
    "                idx = torch.randperm(features.size(0))[:num_samples]\n",
    "                features = features[idx]\n",
    "            all_features.append(features.cpu().numpy())\n",
    "    all_features = np.concatenate(all_features, axis=0)\n",
    "    return all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_clustering_dbscan(model, dataloader, device, eps=0.5, min_samples=5, sample_fraction=1.0):\n",
    "    \"\"\"\n",
    "    Применяет DBSCAN к dense признакам, извлечённым из всего датасета, и вычисляет:\n",
    "      - silhouette score,\n",
    "      - Davies-Bouldin index,\n",
    "      - Calinski-Harabasz score.\n",
    "    \"\"\"\n",
    "    features = extract_dense_features_global(model, dataloader, device, sample_fraction)\n",
    "    print(f\"Total extracted features: {features.shape[0]}\")\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    cluster_labels = dbscan.fit_predict(features)\n",
    "    \n",
    "    unique_clusters = set(cluster_labels)\n",
    "    if -1 in unique_clusters:\n",
    "        unique_clusters.remove(-1)\n",
    "    \n",
    "    if len(unique_clusters) < 2:\n",
    "        print(\"Not enough clusters (>=2 required) in global clustering.\")\n",
    "        return None, None, None, cluster_labels\n",
    "    \n",
    "    sil = silhouette_score(features, cluster_labels)\n",
    "    db_index = davies_bouldin_score(features, cluster_labels)\n",
    "    ch_score = calinski_harabasz_score(features, cluster_labels)\n",
    "    return sil, db_index, ch_score, cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dense_features_per_image(model, image, device):\n",
    "    \"\"\"\n",
    "    Извлекает dense признаки для одного изображения, возвращает массив размерности [H*W, C].\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        image = image.unsqueeze(0).to(device)  # [1, 3, H, W]\n",
    "        feat_map = model.backbone(image)[-1]     # [1, C, H, W]\n",
    "        feat_map = F.normalize(feat_map, p=2, dim=1)\n",
    "        _, C, H, W = feat_map.shape\n",
    "        features = feat_map.view(1, C, -1).permute(0, 2, 1).contiguous().view(-1, C)\n",
    "    return features.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dense_features_per_image(model, image, device):\n",
    "    \"\"\"\n",
    "    Извлекает dense признаки для одного изображения, возвращает массив размерности [H*W, C].\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        image = image.unsqueeze(0).to(device)  # [1, 3, H, W]\n",
    "        feat_map = model.backbone(image)[-1]     # [1, C, H, W]\n",
    "        feat_map = F.normalize(feat_map, p=2, dim=1)\n",
    "        _, C, H, W = feat_map.shape\n",
    "        features = feat_map.view(1, C, -1).permute(0, 2, 1).contiguous().view(-1, C)\n",
    "    return features.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_image_clustering_dbscan(model, dataloader, device, eps=0.5, min_samples=5):\n",
    "    \"\"\"\n",
    "    Для каждого изображения выполняет DBSCAN кластеризацию dense признаков и вычисляет метрики:\n",
    "      silhouette score, Davies-Bouldin и Calinski-Harabasz.\n",
    "    Усредняет метрики по всем изображениям, где удалось получить >=2 кластера.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    sil_scores, db_scores, ch_scores = [], [], []\n",
    "    image_count = 0\n",
    "    for images, _ in dataloader:\n",
    "        for i in range(images.size(0)):\n",
    "            features = extract_dense_features_per_image(model, images[i], device)\n",
    "            dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "            labels = dbscan.fit_predict(features)\n",
    "            unique_clusters = set(labels)\n",
    "            if -1 in unique_clusters:\n",
    "                unique_clusters.remove(-1)\n",
    "            if len(unique_clusters) < 2:\n",
    "                continue\n",
    "            try:\n",
    "                sil = silhouette_score(features, labels)\n",
    "                db = davies_bouldin_score(features, labels)\n",
    "                ch = calinski_harabasz_score(features, labels)\n",
    "                sil_scores.append(sil)\n",
    "                db_scores.append(db)\n",
    "                ch_scores.append(ch)\n",
    "                image_count += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Error on image {image_count}: {e}\")\n",
    "                continue\n",
    "    if len(sil_scores) == 0:\n",
    "        print(\"No image produced enough clusters for per-image evaluation.\")\n",
    "        return None, None, None\n",
    "    avg_sil = np.mean(sil_scores)\n",
    "    avg_db = np.mean(db_scores)\n",
    "    avg_ch = np.mean(ch_scores)\n",
    "    print(f\"Processed {image_count} images. Avg Silhouette: {avg_sil:.4f}, Avg Davies-Bouldin: {avg_db:.4f}, Avg Calinski-Harabasz: {avg_ch:.4f}\")\n",
    "    return avg_sil, avg_db, avg_ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(source_dir, train_dir, val_dir, train_ratio=0.8, extensions=('.png', '.jpg', '.jpeg', '.bmp')):\n",
    "    \"\"\"\n",
    "    Разбивает файлы из source_dir на тренировочный и валидационный наборы и копирует их в train_dir и val_dir.\n",
    "    \n",
    "    Args:\n",
    "        source_dir (str): путь к исходной папке с изображениями.\n",
    "        train_dir (str): путь к папке, куда будут скопированы тренировочные изображения.\n",
    "        val_dir (str): путь к папке, куда будут скопированы валидационные изображения.\n",
    "        train_ratio (float): доля изображений, которые пойдут в тренировочный набор.\n",
    "        extensions (tuple): допустимые расширения файлов.\n",
    "    \"\"\"\n",
    "    # Создаем папки, если их нет\n",
    "    os.makedirs(train_dir, exist_ok=True)\n",
    "    os.makedirs(val_dir, exist_ok=True)\n",
    "    \n",
    "    # Получаем список файлов с нужными расширениями\n",
    "    all_files = [f for f in os.listdir(source_dir) if f.lower().endswith(extensions)]\n",
    "    print(f\"Найдено {len(all_files)} изображений в папке {source_dir}.\")\n",
    "    \n",
    "    # Перемешиваем список случайным образом\n",
    "    random.shuffle(all_files)\n",
    "    \n",
    "    # Определяем индекс для разбиения\n",
    "    split_index = int(len(all_files) * train_ratio)\n",
    "    train_files = all_files[:split_index]\n",
    "    val_files = all_files[split_index:]\n",
    "    \n",
    "    # Копируем файлы в соответствующие папки\n",
    "    for filename in train_files:\n",
    "        shutil.copy(os.path.join(source_dir, filename), os.path.join(train_dir, filename))\n",
    "    \n",
    "    for filename in val_files:\n",
    "        shutil.copy(os.path.join(source_dir, filename), os.path.join(val_dir, filename))\n",
    "    \n",
    "    print(f\"Тренировочный набор: {len(train_files)} изображений.\")\n",
    "    print(f\"Валидационный набор: {len(val_files)} изображений.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageFolderDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Пользовательский датасет для папки с изображениями.\n",
    "    Все файлы с расширениями .png, .jpg, .jpeg, .bmp будут загружены.\n",
    "    Так как данные не размечены, возвращается фиктивная метка (0).\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (str): путь к папке с изображениями.\n",
    "            transform (callable, optional): Трансформации, которые применяются к изображению.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = [\n",
    "            os.path.join(root_dir, file)\n",
    "            for file in os.listdir(root_dir)\n",
    "            if file.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))\n",
    "        ]\n",
    "        self.image_files = sorted(self.image_files)  # Опционально сортируем файлы\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        # Возвращаем изображение и фиктивную метку\n",
    "        return image, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "    #                      std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = 'dataset/turbine'\n",
    "train_folder = 'dataset/turbine_train'\n",
    "val_folder = 'dataset/turbine_val'\n",
    "\n",
    "# split_dataset(source_dir=data_folder, train_dir=train_folder, val_dir=val_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImageFolderDataset(root_dir=train_folder, transform=base_transform)\n",
    "val_dataset   = ImageFolderDataset(root_dir=val_folder, transform=base_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.compute_mean_std import compute_mean_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_mean_std(train_dataset, 32, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "task = Task.init(project_name=\"SSL_Detection\", task_name=\"PixPro Training \")\n",
    "\n",
    "# imagenet_model = timm.create_model('resnet18', pretrained=True, features_only=True)\n",
    "model = PixPro(backbone_name='resnet18').to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "num_epochs = 100\n",
    "val_interval = 5\n",
    "save_interval = 10\n",
    "eps = 0.5\n",
    "min_samples = 5\n",
    "sample_fraction = 0.2\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    avg_loss = train_pixpro(model, train_loader, optimizer, device, epoch, augment_fn=batch_augmentations)\n",
    "    task.get_logger().report_scalar(\"Training\", \"Loss\", iteration=epoch, value=avg_loss)\n",
    "    \n",
    "    # Проводим валидацию каждые val_interval эпох\n",
    "    if epoch % val_interval == 0:\n",
    "        print(f\"--- Validation at Epoch {epoch} ---\")\n",
    "        # Глобальная кластеризация DBSCAN\n",
    "        global_results = global_clustering_dbscan(model, val_loader, device, eps, min_samples, sample_fraction)\n",
    "        if global_results[0] is not None:\n",
    "            sil, db_index, ch_score, _ = global_results\n",
    "            print(f\"Global DBSCAN -> Silhouette: {sil:.4f}, Davies-Bouldin: {db_index:.4f}, Calinski-Harabasz: {ch_score:.4f}\")\n",
    "            task.get_logger().report_scalar(\"Clustering_Sil\", \"Global_Silhouette\", iteration=epoch, value=sil)\n",
    "            task.get_logger().report_scalar(\"Clustering_DB\", \"Global_Davies_Bouldin\", iteration=epoch, value=db_index)\n",
    "            task.get_logger().report_scalar(\"Clustering_CH\", \"Global_Calinski_Harabasz\", iteration=epoch, value=ch_score)\n",
    "        else:\n",
    "            print(\"Global clustering did not produce enough clusters.\")\n",
    "        \n",
    "        # Кластеризация по отдельности для каждого изображения\n",
    "        per_img_results = per_image_clustering_dbscan(model, val_loader, device, eps, min_samples)\n",
    "        if per_img_results[0] is not None:\n",
    "            avg_sil, avg_db, avg_ch = per_img_results\n",
    "            print(f\"Per-image DBSCAN -> Avg Silhouette: {avg_sil:.4f}, Avg Davies-Bouldin: {avg_db:.4f}, Avg Calinski-Harabasz: {avg_ch:.4f}\")\n",
    "            task.get_logger().report_scalar(\"Clustering_Sil\", \"PerImage_Silhouette\", iteration=epoch, value=avg_sil)\n",
    "            task.get_logger().report_scalar(\"Clustering_DB\", \"PerImage_Davies_Bouldin\", iteration=epoch, value=avg_db)\n",
    "            task.get_logger().report_scalar(\"Clustering_CH\", \"PerImage_Calinski_Harabasz\", iteration=epoch, value=avg_ch)\n",
    "        else:\n",
    "            print(\"Per-image clustering did not produce metrics for enough images.\")\n",
    "    \n",
    "\n",
    "    if epoch % save_interval == 0:\n",
    "        path_to_ckpt = f'checkpoint_epoch_{epoch}.pth'\n",
    "        torch.save(model.state_dict(), path_to_ckpt)\n",
    "        task.upload_artifact(name=\"model_checkpoint\", artifact_object=path_to_ckpt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".ssl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
